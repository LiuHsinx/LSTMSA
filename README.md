### 项目流程：
制作词向量，可以用gensim库，也可以用训练好的Word2Vec模型
词和ID的映射
构建LSTM网络架构
训练模型
结果分析
#### 数据：
为了简单起见，我们使用Google在大规模数据集上训练好的Word2Vec 模型来创建词向量。在这个模型中，谷歌能创建 300 万个词向量，每个向量维度为 300。但因为这个单词向量矩阵相当大（3.6G），我们用另外一个由 GloVe 进行训练到的小矩阵。矩阵将包含 400000 个词向量，每个向量的维数为 50。我们将导入两个不同的数据结构，一个是包含 400000 个单词的 Python 列表（wordList），一个是包含所有单词向量值得 400000*50 维的嵌入矩阵(wordVectors)。

训练集我们使用的是 IMDB 数据集。这个数据集包含 25000 条电影数据，其中 12500 条正向数据，12500 条负向数据。正向数据包含在一个文本文件中，负向数据包含在另一个文本文件中。

#### LSTM模型构建：
开始构建 TensorFlow 图模型：使用 TensorFlow 的嵌入函数，其有两个参数，一个是嵌入矩阵（在我们的情况下是词向量矩阵），另一个是每个词对应的索引。模型输入占位符是一个整数化的索引数组。标签占位符代表一组值，每一个值都为 [1,0] 或者 [0,1]，这个取决于数据是正向的还是负向的。设置输入数据占位符后，调用 tf.nn.embedding_lookup() 函数来得到词向量。该函数将返回一个三维向量，第一个维度是批处理大小，第二个维度是句子长度，第三个维度是词向量长度。

现在已经得到了我们想要的数据形式，接下来将这种数据形式输入到 LSTM 网络中。首先使用 tf.nn.rnn_cell.BasicLSTMCell 函数，其输入的参数是一个整数，表示需要几个 LSTM 单元。这是一个超参数，需要对这个数值进行调试从而找到最优的解。然后设置 dropout 参数避免过拟合。最后将 LSTM cell 和三维的数据输入到 tf.nn.dynamic_rnn ，这个函数的功能是展开整个网络，并且构建一整个 RNN 模型。 dynamic RNN 函数的第一个输出可以被认为是最后的隐藏状态向量。这个向量将被重新确定维度，然后乘以最后的权重矩阵和一个偏置项来获得最终的输出值（全连接层）。

接下来定义正确的预测函数和正确率评估参数。正确的预测形式是查看最后输出的0-1向量是否和标记的0-1向量相同。 使用标准的交叉熵损失函数来作为目标函数。优化器选择 Adam，并且采用默认的学习率。

#### 预训练结果：

![image](https://github.com/LiuHsinx/LSTMSA/assets/93762973/8e320605-e942-42e0-a136-4ee8617a9740)

#### 训练结束后导入预训练模型验证在测试集上的结果：
最高batch可达96%左右

![image](https://github.com/LiuHsinx/LSTMSA/assets/93762973/73504b29-c238-4cdc-a531-798493c245d6)


