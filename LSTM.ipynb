{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c4bc74",
   "metadata": {},
   "source": [
    "### 项目流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ffdc",
   "metadata": {},
   "source": [
    "1) 制作词向量，可以用gensim库，也可以用训练好的Word2Vec模型\n",
    "\n",
    "2) 词和ID的映射\n",
    "\n",
    "3) 构建LSTM网络架构\n",
    "\n",
    "4) 训练模型\n",
    "\n",
    "5) 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e43c8",
   "metadata": {},
   "source": [
    "#### 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b38f7",
   "metadata": {},
   "source": [
    "首先创建词向量：为了简单起见，我们使用Google在大规模数据集上训练好的Word2Vec 模型来创建。\n",
    "\n",
    "在这个模型中，谷歌能创建 300 万个词向量，每个向量维度为 300。\n",
    "\n",
    "但因为这个单词向量矩阵相当大（3.6G），我们用另外一个由 GloVe 进行训练到的小矩阵。\n",
    "\n",
    "矩阵将包含 400000 个词向量，每个向量的维数为 50。\n",
    "\n",
    "我们将导入两个不同的数据结构，一个是包含 400000 个单词的 Python 列表（wordList），一个是包含所有单词向量值得 400000*50 维的嵌入矩阵(wordVectors)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47de97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordList = np.load('./data/training_data/wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordList = wordList.tolist()   # originally loaded as numpy array\n",
    "wordList = [word.decode('UTF-8') for word in wordList]  # encode word as UFF-8\n",
    "wordVectors = np.load('./data/training_data/wordVectors.npy')\n",
    "print('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0bfaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordList))   # 每个英文单词对应一个id的映射\n",
    "print(wordVectors.shape)   # 词向量维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ecddf",
   "metadata": {},
   "source": [
    "训练集我们使用的是 IMDB 数据集。这个数据集包含 25000 条电影数据，其中 12500 条正向数据，12500 条负向数据。正向数据包含在一个文本文件中，负向数据包含在另一个文本文件中。\n",
    "\n",
    "在训练集上构造索引前，我们可先计算或可视化每个文件中单词的平均长度。这将帮助去决定如何设置最大序列长度的最佳值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59dfe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 25000\n",
      "The total number of words in the files is 5844680\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['./data/training_data/positiveReviews/' + f for f in listdir('./data/training_data/positiveReviews/') if isfile(join('./data/training_data/positiveReviews/', f))]\n",
    "negativeFiles = ['./data/training_data/negativeReviews/' + f for f in listdir('./data/training_data/negativeReviews/') if isfile(join('./data/training_data/negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())  # 空格分割\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f656c",
   "metadata": {},
   "source": [
    "从句子的平均单词数，我们认为将句子最大长度设置为 250 是可行的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fa0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ef43b",
   "metadata": {},
   "source": [
    "接下来我们将全部的 25000 条评论文件中的文本转换成索引矩阵（ 25000 * 250 ），即每个单词对应wordList中的一个索引\n",
    "\n",
    "其中这些文本我们需要做数据清洗（预处理）操作，比如删除标点符号、去停用词等，只留下字母和数字字符\n",
    "\n",
    "这是一个计算成本非常高的过程，可以直接使用处理好的索引矩阵文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42528853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# for pf in positiveFiles:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "\n",
    "# for nf in negativeFiles:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "\n",
    "# np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fb8c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 250)\n"
     ]
    }
   ],
   "source": [
    "ids = np.load('./data/training_data/idsMatrix.npy')\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1849f79",
   "metadata": {},
   "source": [
    "#### 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0fbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727252f",
   "metadata": {},
   "source": [
    "### LSTM模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b8924",
   "metadata": {},
   "source": [
    "开始构建 TensorFlow 图模型：使用 TensorFlow 的嵌入函数，其有两个参数，一个是嵌入矩阵（在我们的情况下是词向量矩阵），另一个是每个词对应的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e01b0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24   # 批处理大小\n",
    "lstmUnits = 64  # 隐藏层神经元个数\n",
    "numClasses = 2   # 二分类情感分析\n",
    "iterations = 50000   # 迭代次数\n",
    "numDimensions = 300 #每个单词向量的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf27d8",
   "metadata": {},
   "source": [
    "模型输入占位符是一个整数化的索引数组。\n",
    "\n",
    "标签占位符代表一组值，每一个值都为 [1,0] 或者 [0,1]，这个取决于数据是正向的还是负向的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97bf1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])   # 标签占位符\n",
    "input_data = tf.placeholder(tf.32, [batchSize, maxSeqLength])  # 输入数据占位符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf06612",
   "metadata": {},
   "source": [
    "设置输入数据占位符后，调用 tf.nn.embedding_lookup() 函数来得到词向量。\n",
    "\n",
    "该函数将返回一个三维向量，第一个维度是批处理大小，第二个维度是句子长度，第三个维度是词向量长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5afa0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52238cc2",
   "metadata": {},
   "source": [
    "现在已经得到了我们想要的数据形式，接下来将这种数据形式输入到 LSTM 网络中。\n",
    "\n",
    "首先使用 tf.nn.rnn_cell.BasicLSTMCell 函数，其输入的参数是一个整数，表示需要几个 LSTM 单元。这是一个超参数，需要对这个数值进行调试从而找到最优的解。然后设置 dropout 参数避免过拟合。最后将 LSTM cell 和三维的数据输入到 tf.nn.dynamic_rnn ，这个函数的功能是展开整个网络，并且构建一整个 RNN 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eed810c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py37_tf\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "lstmCell = tf.nn.rnn_cell.LSTMCell(lstmUnits)   # 传入64个神经元\n",
    "lstmCell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)   # 自定义的dropout\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)   # 模型展开"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8a44c",
   "metadata": {},
   "source": [
    "dynamic RNN 函数的第一个输出可以被认为是最后的隐藏状态向量。这个向量将被重新确定维度，然后乘以最后的权重矩阵和一个偏置项来获得最终的输出值（全连接层）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "486db789",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "#取最终的结果值\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcf6fc",
   "metadata": {},
   "source": [
    "接下来定义正确的预测函数和正确率评估参数。正确的预测形式是查看最后输出的0-1向量是否和标记的0-1向量相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f19fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f2ae1f",
   "metadata": {},
   "source": [
    "之后使用标准的交叉熵损失函数来作为目标函数。优化器选择 Adam，并且采用默认的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce5baf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c9cb3",
   "metadata": {},
   "source": [
    "### 超参调整"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ed502",
   "metadata": {},
   "source": [
    "训练损失值与选择的优化器（Adam，Adadelta，SGD，等等），学习率和网络架构都有很大的关系。特别是在RNN和LSTM中，单元数量和词向量的大小都是重要因素。\n",
    "\n",
    "学习率：RNN最难的一点就是它的训练非常困难，因为时间步骤很长。那么，学习率就变得非常重要了。如果我们将学习率设置的很大，那么学习曲线就会波动性很大，如果我们将学习率设置的很小，那么训练过程就会非常缓慢。根据经验，将学习率默认设置为 0.001 是一个比较好的开始。如果训练的非常缓慢，那么你可以适当的增大这个值，如果训练过程非常的不稳定，那么你可以适当的减小这个值。\n",
    "\n",
    "优化器：这个在研究中没有一个一致的选择，但是 Adam 优化器被广泛的使用。\n",
    "\n",
    "LSTM单元的数量：这个值很大程度上取决于输入文本的平均长度。而更多的单元数量可以帮助模型存储更多的文本信息，当然模型的训练时间就会增加很多，并且计算成本会非常昂贵。\n",
    "\n",
    "词向量维度：词向量的维度一般我们设置为50到300。维度越多意味着可以存储更多的单词信息，但是需要付出的是更昂贵的计算成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294c554",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7a7ce",
   "metadata": {},
   "source": [
    "基本思路：首先定义一个 TensorFlow 会话。然后加载一批评论和对应的标签。接下来，调用会话的 run 函数。这个函数有两个参数，第一个是 fetches 参数（定义了我们感兴趣的值）。第二个是 feed_dict 参数,就是提供的占位符。\n",
    "\n",
    "目标：通过优化器来最小化损失函数。\n",
    "\n",
    "做法：将一批处理的评论和标签输入模型，然后不断对这一组数据进行循环训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b21c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1001/50000... loss 0.7066070437431335... accuracy 0.5833333134651184...\n",
      "iteration 2001/50000... loss 0.6860122680664062... accuracy 0.375...\n",
      "iteration 3001/50000... loss 0.6711758971214294... accuracy 0.4583333432674408...\n",
      "iteration 4001/50000... loss 0.660219669342041... accuracy 0.625...\n",
      "iteration 5001/50000... loss 0.7247483730316162... accuracy 0.5833333134651184...\n",
      "iteration 6001/50000... loss 0.7750604152679443... accuracy 0.5...\n",
      "iteration 7001/50000... loss 0.675737202167511... accuracy 0.4583333432674408...\n",
      "iteration 8001/50000... loss 0.7090887427330017... accuracy 0.6666666865348816...\n",
      "iteration 9001/50000... loss 0.5143955945968628... accuracy 0.75...\n",
      "iteration 10001/50000... loss 0.556067168712616... accuracy 0.75...\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "iteration 11001/50000... loss 0.6969477534294128... accuracy 0.625...\n",
      "iteration 12001/50000... loss 0.6766869425773621... accuracy 0.7083333134651184...\n",
      "iteration 13001/50000... loss 0.4536910057067871... accuracy 0.8333333134651184...\n",
      "iteration 14001/50000... loss 0.7131029963493347... accuracy 0.625...\n",
      "iteration 15001/50000... loss 0.5721121430397034... accuracy 0.7083333134651184...\n",
      "iteration 16001/50000... loss 0.44227585196495056... accuracy 0.8333333134651184...\n",
      "iteration 17001/50000... loss 0.6484478116035461... accuracy 0.625...\n",
      "iteration 18001/50000... loss 0.3749699592590332... accuracy 0.8333333134651184...\n",
      "iteration 19001/50000... loss 0.6865954399108887... accuracy 0.5...\n",
      "iteration 20001/50000... loss 0.6455555558204651... accuracy 0.7083333134651184...\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "iteration 21001/50000... loss 0.6720466017723083... accuracy 0.7083333134651184...\n",
      "iteration 22001/50000... loss 0.30687451362609863... accuracy 0.875...\n",
      "iteration 23001/50000... loss 0.30744174122810364... accuracy 0.9166666865348816...\n",
      "iteration 24001/50000... loss 0.29197177290916443... accuracy 0.8333333134651184...\n",
      "iteration 25001/50000... loss 0.4525165259838104... accuracy 0.75...\n",
      "iteration 26001/50000... loss 0.5948459506034851... accuracy 0.7916666865348816...\n",
      "iteration 27001/50000... loss 0.3780648708343506... accuracy 0.875...\n",
      "iteration 28001/50000... loss 0.5334206223487854... accuracy 0.75...\n",
      "iteration 29001/50000... loss 0.29894572496414185... accuracy 0.875...\n",
      "iteration 30001/50000... loss 0.3169746994972229... accuracy 0.8333333134651184...\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "iteration 31001/50000... loss 0.3634713590145111... accuracy 0.8333333134651184...\n",
      "iteration 32001/50000... loss 0.28143107891082764... accuracy 0.9166666865348816...\n",
      "iteration 33001/50000... loss 0.4508020579814911... accuracy 0.8333333134651184...\n",
      "iteration 34001/50000... loss 0.16158732771873474... accuracy 0.9583333134651184...\n",
      "iteration 35001/50000... loss 0.23576004803180695... accuracy 0.9583333134651184...\n",
      "iteration 36001/50000... loss 0.49363017082214355... accuracy 0.8333333134651184...\n",
      "iteration 37001/50000... loss 0.8813881278038025... accuracy 0.6666666865348816...\n",
      "iteration 38001/50000... loss 0.15977345407009125... accuracy 0.9166666865348816...\n",
      "iteration 39001/50000... loss 0.30976539850234985... accuracy 0.875...\n",
      "iteration 40001/50000... loss 0.37355828285217285... accuracy 0.7916666865348816...\n",
      "saved to models/pretrained_lstm.ckpt-40000\n",
      "iteration 41001/50000... loss 0.7141516804695129... accuracy 0.75...\n",
      "iteration 42001/50000... loss 0.29954925179481506... accuracy 0.8333333134651184...\n",
      "iteration 43001/50000... loss 0.6392157673835754... accuracy 0.7916666865348816...\n",
      "iteration 44001/50000... loss 0.25419947504997253... accuracy 0.9166666865348816...\n",
      "iteration 45001/50000... loss 1.1921390295028687... accuracy 0.6666666865348816...\n",
      "iteration 46001/50000... loss 0.5665101408958435... accuracy 0.875...\n",
      "iteration 47001/50000... loss 1.201157569885254... accuracy 0.75...\n",
      "iteration 48001/50000... loss 0.5371384024620056... accuracy 0.7916666865348816...\n",
      "iteration 49001/50000... loss 0.6704383492469788... accuracy 0.9166666865348816...\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels}) \n",
    "    \n",
    "    if (i % 1000 == 0 and i != 0):\n",
    "        nextBatch, nextBatchLabels = getTestBatch();\n",
    "        loss_ = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        accuracy_ = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        print(\"iteration {}/{}...\".format(i+1, iterations),\n",
    "              \"loss {}...\".format(loss_),\n",
    "              \"accuracy {}...\".format(accuracy_))  \n",
    "        \n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b65e35",
   "metadata": {},
   "source": [
    "导入一个预训练的模型需要使用 TensorFlow 的另一个会话函数，称为 Saver ，然后利用这个会话函数来调用 restore 函数。这个函数包括两个参数，一个表示当前的会话，另一个表示保存的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51778475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\pretrained_lstm.ckpt-40000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()   # 实例化\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))   # 根据时间记录找出最终模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6298a",
   "metadata": {},
   "source": [
    "然后从测试集中导入一些电影评论。请注意，这些评论是模型从来没有看见过的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6e0e08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 87.5\n",
      "Accuracy for this batch: 87.5\n",
      "Accuracy for this batch: 95.83333134651184\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n",
      "Accuracy for this batch: 66.66666865348816\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933566ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
